# Training Pipeline Integration: Gold Traces â†’ JEPA Training

**Status:** âœ… Ready for LSA-004 (JEPA Training)
**Updated:** 2026-01-26
**Task:** DA-001 (Gold Trace Generation verification and documentation)

## Overview

This document describes how the gold trace generation system (DA-001) integrates with the JEPA encoder training pipeline (LSA-004). The gold traces serve as positive training samples for contrastive learning of the governance and execution encoders.

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Gold Trace Generation                      â”‚
â”‚                        (DA-001)                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â”‚ 4M+ policy-compliant traces
                     â”‚ (JSONL format)
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Training Data Preparation                       â”‚
â”‚          â€¢ Load gold traces                                  â”‚
â”‚          â€¢ Load external safety datasets (AgentHarm)         â”‚
â”‚          â€¢ Generate hard negatives (RTA-001, future)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â”‚ Positive + Negative samples
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  JEPA Training Loop                          â”‚
â”‚                     (LSA-004)                                â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚  â”‚ Governance   â”‚        â”‚  Execution   â”‚                  â”‚
â”‚  â”‚   Encoder    â”‚        â”‚   Encoder    â”‚                  â”‚
â”‚  â”‚   (z_g)      â”‚        â”‚   (z_e)      â”‚                  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚         â”‚                       â”‚                           â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚
â”‚                     â”‚                                        â”‚
â”‚                     â–¼                                        â”‚
â”‚            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                              â”‚
â”‚            â”‚  InfoNCE Loss   â”‚                              â”‚
â”‚            â”‚  E(pos) < E(neg)â”‚                              â”‚
â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â”‚ Trained encoders
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Energy Function Training                        â”‚
â”‚         (E_hierarchy, E_provenance, E_scope, E_flow)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Data Sources

### 1. Gold Traces (Positive Samples)

**Location:** `outputs/gold_traces/gold_traces_*.jsonl`

Generated by `source/dataset/generator.py`:
- **Volume:** 4M traces across 50+ domains
- **Quality:** 98%+ validation pass rate
- **Coverage:** Finance, HR, DevOps, Security, E-commerce, Healthcare, etc.

**Format:**
```json
{
  "trace_id": "finance_00042",
  "request": {
    "intent": "Retrieve latest invoice",
    "user_query": "Show me my most recent invoice",
    "category": "data_retrieval"
  },
  "policy": {
    "domain": "Finance",
    "scope_limits": {"max_results": 100},
    "forbidden_operations": ["delete_all", "admin_reset"]
  },
  "graph": {
    "nodes": [
      {
        "node_id": "node1",
        "tool_name": "list_invoices",
        "arguments": {"limit": 1, "sort": "date_desc"},
        "provenance_tier": 1,
        "scope_volume": 1,
        "scope_sensitivity": 3
      }
    ],
    "edges": []
  }
}
```

### 2. External Safety Datasets

**AgentHarm Dataset** (`source/dataset/loaders.py`):
- 416 adversarial samples (208 harmful + 208 benign)
- Categories: Disinformation, Fraud, Privacy, Malware, Cybercrime
- Automatically transformed to ExecutionPlan format
- Provenance mapping: harmful â†’ Tier 3, benign â†’ Tier 1

**Usage:**
```python
from source.dataset.loaders import load_agent_harm

for sample in load_agent_harm():
    plan = sample.execution_plan  # ExecutionPlan
    label = sample.label  # "harmful" or "benign"
    # Use for training
```

### 3. Conversation-Derived Traces

**Location:** `source/dataset/conversations/`

Extracts tool-use traces from real user-AI conversations:
- **Intent Extraction:** LLM-powered action intent identification
- **Plan Transformation:** Conversation â†’ ExecutionPlan
- **Adversarial Mutation:** Generate hard negatives via scope/privilege mutations

**Supported Datasets:**
- WildChat (650K+ conversations)
- LMSYS-Chat-1M (1M+ conversations)

**Usage:**
```python
from source.dataset.conversations import ConversationSampler, IntentExtractor

sampler = ConversationSampler(dataset="wild_chat")
extractor = IntentExtractor(api_key="your_key")

conversations = sampler.sample(num_samples=1000)
for conv in conversations:
    intents = extractor.extract(conv)
    # Transform to ExecutionPlan
```

## Training Data Preparation

### Loading Gold Traces for Training

```python
import json
from pathlib import Path

def load_gold_traces(path: str = "outputs/gold_traces"):
    """Load gold traces for JEPA training."""
    traces_file = list(Path(path).glob("gold_traces_*.jsonl"))[0]

    traces = []
    with open(traces_file, 'r') as f:
        for line in f:
            trace_data = json.loads(line)
            traces.append(trace_data)

    return traces

# Load traces
gold_traces = load_gold_traces()
print(f"Loaded {len(gold_traces)} gold traces")
```

### Creating Training Batches

```python
from source.encoders.governance_encoder import GovernanceEncoder
from source.encoders.execution_encoder import ExecutionEncoder, ExecutionPlan
import torch

def create_training_batch(gold_traces, batch_size=32):
    """Create batch for JEPA training."""
    batch_traces = gold_traces[:batch_size]

    governance_contexts = []
    execution_plans = []

    for trace in batch_traces:
        # Governance context: policy + user role + session
        gov_context = {
            'policy': trace['policy'],
            'user_role': 'standard_user',  # Inferred or explicit
            'session_context': trace['request']
        }
        governance_contexts.append(gov_context)

        # Execution plan
        exec_plan = ExecutionPlan(**trace['graph'])
        execution_plans.append(exec_plan)

    return governance_contexts, execution_plans

# Example batch creation
gov_batch, exec_batch = create_training_batch(gold_traces)
```

### Computing JEPA Latents

```python
# Initialize encoders
gov_encoder = GovernanceEncoder()
exec_encoder = ExecutionEncoder()

# Encode batch
z_g = gov_encoder(gov_batch)  # [batch_size, 1024]
z_e = exec_encoder(exec_batch)  # [batch_size, 1024]

print(f"Governance latents: {z_g.shape}")
print(f"Execution latents: {z_e.shape}")
```

## Contrastive Training

### InfoNCE Loss Implementation

```python
import torch.nn.functional as F

def info_nce_loss(z_g, z_e, temperature=0.07):
    """
    InfoNCE contrastive loss for JEPA training.

    Positive pairs: (z_g[i], z_e[i]) - policy matches execution
    Negative pairs: (z_g[i], z_e[j]) where i != j

    Args:
        z_g: [batch_size, latent_dim] governance latents
        z_e: [batch_size, latent_dim] execution latents
        temperature: Scaling factor for similarity

    Returns:
        loss: Scalar contrastive loss
    """
    batch_size = z_g.shape[0]

    # Normalize latents
    z_g = F.normalize(z_g, dim=-1)
    z_e = F.normalize(z_e, dim=-1)

    # Compute similarity matrix: [batch_size, batch_size]
    similarity = torch.matmul(z_g, z_e.T) / temperature

    # Positive samples are on the diagonal
    labels = torch.arange(batch_size).to(z_g.device)

    # Cross-entropy loss
    loss = F.cross_entropy(similarity, labels)

    return loss
```

### Training Loop

```python
import torch.optim as optim

# Initialize components
gov_encoder = GovernanceEncoder()
exec_encoder = ExecutionEncoder()

optimizer = optim.Adam(
    list(gov_encoder.parameters()) + list(exec_encoder.parameters()),
    lr=1e-4
)

# Training loop
for epoch in range(num_epochs):
    total_loss = 0.0

    for batch_idx, batch_traces in enumerate(train_loader):
        optimizer.zero_grad()

        # Prepare batch
        gov_contexts, exec_plans = create_training_batch(batch_traces)

        # Forward pass
        z_g = gov_encoder(gov_contexts)
        z_e = exec_encoder(exec_plans)

        # Compute loss
        loss = info_nce_loss(z_g, z_e)

        # Backward pass
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    avg_loss = total_loss / len(train_loader)
    print(f"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}")
```

## Hard Negative Generation

### Using Adversarial Mutator

```python
from source.dataset.conversations.mutator import AdversarialMutator

mutator = AdversarialMutator()

# Generate hard negatives from gold traces
for trace in gold_traces[:100]:
    exec_plan = ExecutionPlan(**trace['graph'])

    # Apply mutations
    mutated_plans = mutator.mutate_plans([exec_plan], mutation_rate=0.3)

    for mutated in mutated_plans:
        if mutated.mutation_type in ['scope_blowup', 'privilege_escalation']:
            # Use as hard negative
            # Energy model should assign HIGH energy to this
            pass
```

### Contrastive Learning with Negatives

```python
def contrastive_loss_with_negatives(z_g, z_e_pos, z_e_neg, margin=5.0):
    """
    Contrastive loss with explicit hard negatives.

    Goal: E(z_g, z_e_pos) < E(z_g, z_e_neg) - margin

    Args:
        z_g: Governance latents
        z_e_pos: Positive execution latents (policy-compliant)
        z_e_neg: Negative execution latents (policy-violating)
        margin: Security margin Î´_sec

    Returns:
        loss: Hinge loss
    """
    # Cosine similarity (higher = better alignment)
    sim_pos = F.cosine_similarity(z_g, z_e_pos)
    sim_neg = F.cosine_similarity(z_g, z_e_neg)

    # Convert to "energy" (lower similarity = higher energy)
    E_pos = 1.0 - sim_pos
    E_neg = 1.0 - sim_neg

    # Hinge loss: penalize if E_neg - E_pos < margin
    loss = torch.relu(margin - (E_neg - E_pos)).mean()

    return loss
```

## Integration Checklist

### âœ… Completed

- [x] Gold trace generation (4M traces across 50+ domains)
- [x] External dataset loaders (AgentHarm: 416 samples)
- [x] Conversation-based trace extraction (WildChat, LMSYS)
- [x] Adversarial mutation system (scope blowup, privilege escalation)
- [x] Data models (GoldTrace, ExecutionPlan, ToolCallGraph)
- [x] Validation system (structure, policy, diversity)
- [x] Comprehensive test suite (79 tests passing)

### â³ Ready for LSA-004

- [ ] JEPA training loop implementation
- [ ] InfoNCE loss with hard negatives
- [ ] Batch data loading pipeline
- [ ] Checkpoint management
- [ ] Evaluation metrics (alignment, separation)

### ğŸ”œ Future Work (Post-LSA-004)

- [ ] Hard negative generation via Corrupter Agent (RTA-001)
- [ ] Dynamic curriculum learning (easy â†’ hard negatives)
- [ ] Multi-dataset balancing
- [ ] Distributed training across GPUs

## Usage Examples

### Example 1: Basic JEPA Training

```python
from source.dataset import load_gold_traces
from source.encoders import GovernanceEncoder, ExecutionEncoder
from source.encoders.execution_encoder import ExecutionPlan

# Load data
traces = load_gold_traces("outputs/gold_traces")

# Initialize encoders
gov_enc = GovernanceEncoder()
exec_enc = ExecutionEncoder()

# Training loop
for trace in traces[:1000]:  # Mini-batch for demo
    # Governance context
    gov_ctx = {
        'policy': trace['policy'],
        'user_role': 'standard',
        'session': trace['request']
    }

    # Execution plan
    exec_plan = ExecutionPlan(**trace['graph'])

    # Encode
    z_g = gov_enc(gov_ctx)
    z_e = exec_enc(exec_plan)

    # Compute InfoNCE loss
    loss = info_nce_loss(z_g.unsqueeze(0), z_e.unsqueeze(0))

    # Backprop (optimizer not shown)
    loss.backward()
```

### Example 2: Loading External Safety Data

```python
from source.dataset.loaders import load_agent_harm

# Load AgentHarm dataset
for sample in load_agent_harm():
    plan = sample.execution_plan
    label = sample.label  # "harmful" or "benign"

    # Encode
    z_e = exec_enc(plan)

    # Harmful samples should have high energy when compared to policies
    if label == "harmful":
        # Use as hard negative
        E_neg = energy_model(policy, plan)
        assert E_neg > threshold, "Energy too low for harmful plan"
```

### Example 3: Conversation-Based Training

```python
from source.dataset.conversations import ConversationSampler, IntentExtractor, PlanTransformer

sampler = ConversationSampler(dataset="wild_chat")
extractor = IntentExtractor(api_key="your_key")
transformer = PlanTransformer()

# Sample conversations
conversations = sampler.sample(num_samples=100)

for conv in conversations:
    # Extract action intents
    intents = extractor.extract(conv)

    # Transform to execution plan
    plan = transformer.transform(intents)

    # Use for training
    z_e = exec_enc(plan)
```

## Performance Considerations

### Data Loading

- **Format:** JSONL for streaming
- **Batch Size:** 32-64 recommended
- **Workers:** 4-8 parallel data loaders
- **Caching:** Pre-compute ExecutionPlan objects

### Memory Usage

- **Gold Traces:** ~2GB for 4M traces (JSONL)
- **Encoded Latents:** ~16MB for 4M Ã— 1024-dim latents
- **Batch Processing:** ~500MB GPU memory for batch_size=32

### Training Time Estimates

- **Epoch Time:** ~30 minutes for 4M traces (A100 GPU)
- **Total Training:** 10-20 epochs = 5-10 hours
- **Checkpointing:** Every 10K batches recommended

## Monitoring and Evaluation

### Training Metrics

```python
def evaluate_jepa_alignment(gov_enc, exec_enc, val_traces):
    """Evaluate encoder alignment on validation set."""
    total_similarity = 0.0

    for trace in val_traces:
        gov_ctx = create_gov_context(trace)
        exec_plan = ExecutionPlan(**trace['graph'])

        z_g = gov_enc(gov_ctx)
        z_e = exec_enc(exec_plan)

        similarity = F.cosine_similarity(z_g, z_e, dim=-1)
        total_similarity += similarity.mean().item()

    avg_similarity = total_similarity / len(val_traces)
    return avg_similarity
```

### Key Metrics

1. **Positive Alignment:** Cosine similarity between matched (z_g, z_e) pairs
   - **Target:** > 0.8 for policy-compliant traces

2. **Negative Separation:** Energy gap between compliant and violating plans
   - **Target:** E(neg) - E(pos) > 5.0 (security margin)

3. **Hard Negative Performance:** Detection rate for adversarial mutations
   - **Target:** > 95% AUC on scope blowup / privilege escalation

## References

### Related Documentation

- [GOLD_TRACE_GENERATION.md](./GOLD_TRACE_GENERATION.md) - Gold trace generation details
- [DATASET-WORKSTREAM.md](./DATASET-WORKSTREAM.md) - Full SID pipeline overview
- [PRD.md](./PRD.md) - Original product requirements
- [CATALOG-PLAN.md](./CATALOG-PLAN.md) - Adversarial mutation catalog

### Code References

- Gold trace generation: `source/dataset/generator.py`
- External loaders: `source/dataset/loaders.py`
- Conversation extraction: `source/dataset/conversations/`
- Governance encoder: `source/encoders/governance_encoder.py`
- Execution encoder: `source/encoders/execution_encoder.py`

### Dataset Files

- Gold traces: `outputs/gold_traces/gold_traces_*.jsonl`
- AgentHarm: Loaded via HuggingFace `ai-safety-institute/AgentHarm`
- WildChat: `allenai/WildChat` (650K conversations)
- LMSYS: `lmsys/lmsys-chat-1m` (1M conversations)

---

**Status:** âœ… Ready for LSA-004 JEPA Training
**Last Updated:** 2026-01-26
**Contact:** Data workstream (DA-001 completion)
