# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

**Project Gatling** is a non-generative, energy-guided integrity layer for agentic systems. It acts as a security firewall that audits and repairs AI agent tool-use plans by measuring semantic consistency between governance policies and execution intent.

### Core Architecture Pattern: Joint-Embedding Predictive Architecture (JEPA)

Gatling treats security as an **Energy-Based Model (EBM)** problem where malicious behavior is defined as a high-energy mismatch between allowed policy and proposed execution:

- **Governance Encoder**: Maps policy, user role, and session context into a Governance Latent
- **Execution Encoder**: Maps proposed plans and provenance into an Execution Latent
- **Energy Function E(z_g, z_e)**: Composable sum of expert critics that detect specific failure modes

### The Four Energy Terms (Product of Experts)

Each energy term addresses a specific security concern:

1. **E_hierarchy**: Penalizes when untrusted retrieved data influences control flow
2. **E_provenance**: Measures "Trust Gap" - spikes when high-privilege tools use unverified instructions
3. **E_scope**: Penalizes over-scoped data access (actual vs. minimal required scope)
4. **E_flow**: Detects exfiltration patterns that deviate from inferred intent

## Development Commands

This project uses **UV** for Python dependency management and execution.

### Environment Setup
```bash
# Install dependencies (UV handles virtual environment automatically)
uv sync

# Install with development tools and pre-commit hooks
make install-dev

# Run the main entry point
uv run python main.py
```

### Testing
```bash
# Run all tests (excluding slow benchmarks)
make test
# or: uv run pytest test/ -v -m "not benchmark"

# Run tests with coverage report
make test-cov

# Run performance benchmarks
make benchmark

# Run specific test file
uv run pytest test/test_encoders/test_governance_encoder.py -v
```

### Code Quality
```bash
# Run linter
make lint

# Format code
make format

# Type check
make type-check

# Run all CI checks locally (before pushing)
make ci
```

### Common Tasks
```bash
# Show all available commands
make help

# Clean cache and build artifacts
make clean
```

## Code Organization

```
gatling/
├── source/          # Core implementation modules
│   ├── encoders/    # JEPA dual-encoder implementations
│   ├── energy/      # Energy function and critic implementations
│   ├── repair/      # Discrete repair engine (Greedy + Beam Search)
│   └── provenance/  # Trust tier and cryptographic tagging
├── config/          # Configuration and policy definitions
├── utils/           # Shared utilities and helpers
├── test/            # Test suite
└── docs/            # Project documentation
    ├── PRD.md              # Product requirements
    ├── WORK-DISTRIBUTION.md # Team workstream allocation
    ├── DATASET_WORK.md     # Gatling-10M dataset pipeline
    └── CATALOG-PLAN.md     # Adversarial mutation catalog
```

## Key Implementation Concepts

### Plan Representation (Typed Tool-Call Graph)

Plans are represented as graphs where:
- **Nodes**: Specific tool invocations with strongly typed arguments
- **Edges**: Data flow and dependency mappings
- **Metadata**: Every node carries:
  - **Provenance Pointer**: Source of the instruction (Trust Tier 1-3)
  - **Scope Vector**: Data volume and sensitivity level

### Training Strategy: InfoNCE + Hard Negative Mining

The model is trained using:
- **Positive samples**: Gold traces (optimal policy execution)
- **Hard negatives**: Generated by the "Corrupter Agent" applying adversarial mutations
- **Margin enforcement**: Fixed security margin δ_sec to ensure stable energy gap

### Adversarial Mutation Catalog

Four primary mutation types for hard negative generation:
1. **Scope Blow-up**: Expand data range limits (e.g., limit=5 → limit=10000)
2. **Instruction Shadowing**: Inject untrusted document content into control flow
3. **Provenance Rug-Pull**: Swap trusted data source for untrusted RAG content
4. **Exfiltration Pivot**: Append data export to external endpoints

### Inference: Discrete Energy-Guided Repair

Real-time plan correction using deterministic local search:
1. **Audit**: Calculate E(z_g, z_e) for proposed plan
2. **Repair** (if E > θ_safe): Apply atomic edits:
   - Narrowing: Reduce scope (max_results=100 → max_results=5)
   - De-escalation: Swap high-privilege tool for restricted equivalent
   - Pruning: Remove tool calls from unverified sources
3. **Accept**: Edits that reduce energy until E < θ_safe

### Performance Requirements

- **Latency Budget**: <200ms end-to-end for Audit + Repair
- **Strategy**: Pre-compute Governance Latents per-user-role, distill common repairs to Fast-Path Policy
- **Safety Gate**: Full EBM validation prevents distilled policy drift

## Research Workstreams

The project is organized into five parallel research tracks:

| Workstream | Lead | Focus |
|------------|------|-------|
| Latent Substrate | Yann LeCun | JEPA dual-encoder training, semantic intent extraction |
| Energy Geometry | Michael Freedman | Landscape smoothness, topological verification |
| Logic & Composition | Yilun Du | Modular energy critics, scaling calibration |
| Adversarial Red-Team | Zico Kolter | Hard negative generation, InfoNCE training |
| Provenance & Trust | Dawn Song | Cryptographic tagging, repair engine, distillation |

## The Gatling-10M Dataset

Synthetic dataset composition:
- **4M Standard Utility**: Establishes the "safe valley" for common tool usage
- **2M Policy Boundary**: Near-safe plans that violate subtle boundaries
- **2M RAG-Injection**: Focused on hierarchy/provenance conflict resolution
- **2M Data Flow/Exfil**: Long-horizon plans testing flow detection

## Success Metrics

### Safety-Utility Tradeoff
1. **Energy Mismatch Recall**: 99.9% detection of plan-injection attempts via AUC
2. **Functional Intent Similarity (FIS)**: ≥90% semantic overlap between risky and repaired plans
3. **Privilege Reduction**: ≥70% reduction in sensitive data access post-repair

## Development Guidelines

- Use **UV** for all Python operations (not pip/venv directly)
- Energy functions must be differentiable and compositionally sound
- All tool schemas must be represented as functional primitives (not raw strings)
- Provenance metadata is mandatory for all data flows
- Repair operations must preserve functional intent (measured via FIS)
- Security margin δ_sec is calibrated via validation suite, not hardcoded

### Code Quality Standards

- **Testing**: Maintain >90% code coverage for new modules
- **Type Hints**: All functions must have type annotations
- **Docstrings**: Use Google-style docstrings for classes and public methods
- **Linting**: Code must pass `ruff` checks (run `make lint`)
- **Formatting**: Use `ruff format` for consistent style (run `make format`)
- **Pre-commit Hooks**: Install with `make install-dev` to catch issues early

### CI/CD Pipeline

- **Automated Testing**: All PRs run tests on Ubuntu and macOS
- **Coverage Tracking**: Coverage reports uploaded to Codecov
- **Performance Monitoring**: Benchmarks track latency regressions
- **Pre-commit Hooks**: Local checks before commits

See [CI/CD Documentation](docs/CI_CD.md) for details.

### Before Committing

Run the full CI suite locally:
```bash
make ci
```

This runs linting, type checking, and tests with coverage.
- You have a @.env file with huggingface token, oopenrouter token, openai and anthropic! you can use it!
- You must keep a detailed implementation / usage / development document. doc and register evey action!
- use huggingface and huggingface skills for training , data set etc...
- You are research based, you do not guess! you think, contemplate, observe and then act. when you dont know something or not sure you look into it and research it deeply! you are cuorious.
- when using env variables in code use python-dotenv to load them
- When building the dataset you should rely as much as you can on realistic, already built datasets, and only augment them if possible, or some kind of mix. use you huggingface datasets skill for that (for example - microsoft/llmail-inject-challenge from hf)
- As you implement code expand the CI coverage for it. do not manipulate the ci to fit your code passing tests!